Here's a summary of the section on **Big O time complexity** and its comparison, written in markdown format:

```markdown
# Wrapping Up Big O Time Complexity

In this section, we will compare various time complexities and how they grow relative to each other. Understanding these comparisons is crucial for analyzing the efficiency of different algorithms.

## Time Complexity Comparison

Let's consider `n = 100` and see how different time complexities behave:

- **O(1)** is always constant, so it remains **1**.
- **O(log n)**: For `n = 100`, this is approximately **7**.
- **O(n)**: For `n = 100`, it equals **100**.
- **O(n²)**: For `n = 100`, it equals **10,000**.

As you can see, there's already a huge difference between these time complexities.

Now, let's increase `n` to 1,000:

- **O(1)** remains **1**.
- **O(log n)**: For `n = 1000`, it's approximately **10** (since 2^10 ≈ 1000).
- **O(n)**: For `n = 1000`, it equals **1,000**.
- **O(n²)**: For `n = 1000`, it equals **1,000,000**.

Again, you can observe how **O(n²)** grows extremely fast compared to the others.

## Big O Terminology

Different time complexities are associated with certain terms:

- **O(n²)**: Often associated with "loop within a loop".
- **O(n)**: Described as "proportional".
- **O(log n)**: Often linked to "divide and conquer".
- **O(1)**: Known as "constant time".

These are the primary time complexities we will focus on in this course.

## Big O Cheat Sheet

There's a useful resource at **bigocheatsheet.com** that provides a detailed breakdown of time and space complexities for various algorithms and data structures. 

### Time Complexity Table for Data Structures

- **Average Case**: Denoted by the Greek letter **theta (θ)**.
- **Worst Case**: Denoted by the Greek letter **omicron (O)**.
- **Space Complexity**: Most data structures have **O(n)** space complexity, except for one, which we'll not cover in this course.

The main takeaway is that **space complexity** for the data structures we cover is generally the same, so we focus more on **time complexity**.

### Sorting Algorithms

Sorting algorithms can have varying time complexities based on different scenarios:

- **Best Case**: Denoted by **omega (Ω)**.
- **Average Case**: Denoted by **theta (θ)**.
- **Worst Case**: Denoted by **O**.

Some algorithms, like **Selection Sort**, have a **poor time complexity** but a **good space complexity**. Others, like **Merge Sort**, have a **great time complexity** but a **poorer space complexity**.

For sorting **numbers**, the best time complexity achievable is **O(n log n)**. However, for other types of data, such as strings, this remains the best possible time complexity.

## Conclusion

This wraps up our section on **Big O notation**. The key takeaway is understanding how different time complexities scale with the size of the input (`n`) and how to make decisions based on that for efficient algorithms. Use the provided resources, like **bigocheatsheet.com**, to further explore these concepts and understand how they apply to various algorithms.
```

This wraps up the understanding of Big O time complexity, with comparisons to help visualize how different algorithms and operations grow as `n` increases. The resource shared is a valuable reference for more in-depth learning.
