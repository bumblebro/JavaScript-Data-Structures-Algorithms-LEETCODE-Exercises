Different terms for inputs is a very popular gotcha technical interview question.

And to explain it, I'm going to bring up some code that we've seen before where we have a for loop

followed by another for loop.

And if you'll recall, each of these ran in times.

And it was O of two n, and we dropped the constant and it became O of n.

So the trick interview question is that instead of passing this in, we're going to pass it two variables.

And the four loops run.

The first one is going to run a times and the second one is going to run B times.

Now here is the kind of knee jerk reaction if you don't know how to do this is well, this one is a

for loop and a single for loop is O of N, and this single for loop is O of n.

And we're going to add these together.

And that becomes o of two n.

And we're going to drop the constant.

And this becomes o of n.

But that is not correct because you can't say that a is equal to n and b is equal to n.

You can't have both variables be equal to n.

Because what if A is one and B is a million?

Those are very different for loops.

So what you have to do is say the first for loop is o of A or whatever the variable might be.

And the second one is going to be o of B.

And when you add these together.

O of A plus B is as far as you can simplify this.

Similarly, if these were nested for loops.

This would become o of a times B.

You can't use N, you actually have to use different terms for inputs.

